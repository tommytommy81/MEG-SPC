{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epochs(raw_filt, detections, tmin=-1, tmax=1, picks_raw=True, no_first_sample=False):\n",
    "    \"\"\" Here we create epochs for events\"\"\"\n",
    "    import mne\n",
    "    import numpy as np\n",
    "    eve_id = 1\n",
    "    eve_name = 'ICA_det'\n",
    "    \n",
    "\n",
    "    raw_filt.load_data()\n",
    "    \n",
    "    new_events, eve = [], []\n",
    "    if no_first_sample:\n",
    "        first_samp = 0\n",
    "    else:\n",
    "        first_samp = raw_filt.first_samp\n",
    "    \n",
    "    for spike_time in detections:\n",
    "        eve = [int(round(spike_time + first_samp)), 0, eve_id]\n",
    "        new_events.append(eve)\n",
    "    \n",
    "    ch_name = 'ICA'\n",
    "    if ch_name not in raw_filt.info['ch_names']:\n",
    "        stim_data = np.zeros((1, len(raw_filt.times)))\n",
    "        info_sp = mne.create_info([ch_name], raw_filt.info['sfreq'], ['stim'])\n",
    "        stim_sp = mne.io.RawArray(stim_data, info_sp, verbose=False)\n",
    "        raw_filt.add_channels([stim_sp], force_update_info=True)\n",
    "\n",
    "    raw_filt.add_events(new_events, stim_channel=ch_name, replace=True)\n",
    "    events= mne.find_events(raw_filt, stim_channel=ch_name, verbose=False)\n",
    "    event_id = {eve_name: eve_id}\n",
    "    picks = mne.pick_types(raw_filt.info, meg=picks_raw, eeg=False, eog=False)\n",
    "    epochs = mne.Epochs(raw_filt, events, event_id,  tmin, tmax, baseline=None, picks=picks, preload=True, verbose=False)\n",
    "    return epochs\n",
    "    del raw_filt, picks, event_id\n",
    "    \n",
    "def find_nearest(array, value):\n",
    "    import numpy as np\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "def plot_topomap(temp_evocked, evocked, svg_path, temp_n, time, amplitude, one_spike=False, diff=0):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mne.viz import plot_evoked_topo\n",
    "    \n",
    "    colors = 'blue','red'\n",
    "    evocked.data = evocked.data*10**13\n",
    "    temp_evocked.data = temp_evocked.data*10**4.5\n",
    "    temp_evocked.times = np.linspace(-0.050,0.050,101) + diff\n",
    "    fig = plot_evoked_topo([evocked,temp_evocked], color=colors, background_color='w',legend=False)\n",
    "    fig.set_figheight(fig.get_figwidth()*1.5)\n",
    "    fig.set_figwidth(fig.get_figheight()*1.5)\n",
    "    if not one_spike:\n",
    "        fig.suptitle('Template %s\\nSpike time: avarage of %s event(s)\\nGoodness of fit (average): %s'%(temp_n, evocked.nave, amplitude), x=0.42)\n",
    "    else:\n",
    "        fig.suptitle('Template %s\\nSpike time: %ss\\nGoodness of fit: %s'%(temp_n, time/1000, amplitude), x=0.42)\n",
    "        temp.one_spike_name = \"spike_%s_templates_topo_%s\"%(time,temp_n)\n",
    "    \n",
    "    fname = \"%s/spike_%s_templates_topo_%s.svg\"%(svg_path,time,temp_n)\n",
    "    fig.savefig(fname, format='svg', papertype='legal')\n",
    "    \n",
    "    temp.templates_topo_temp_n_path = fname\n",
    "    \n",
    "    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "\n",
    "    \n",
    "def plot_joint(evocked):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import mne\n",
    "    fig = mne.viz.plot_evoked_joint(evocked, times = np.array([-0.2, -0.1, -0.05, 0.0, 0.05, 0.1, 0.2]), show=False)\n",
    "    #fig.set_figheight(fig.get_figwidth()*1.05)\n",
    "    #fig.set_figwidth(fig.get_figheight()*1.05)\n",
    "    fname = \"%s%s_evoked_joint_grad.svg\"%(temp.svg_path, temp.temp_n)\n",
    "    plt.savefig(fname, format='svg', papertype='legal')\n",
    "\n",
    "    temp.plot_evoked_joint_temp_n_grad_path = fname\n",
    "    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_epoch_image(epochs):\n",
    "    import mne\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = epochs.plot_image()\n",
    "    fname = \"%s%s_epochs_image.svg\"%(temp.svg_path, temp.temp_n)\n",
    "\n",
    "    fig[0].savefig(fname, format='svg', papertype='legal')\n",
    "    temp.plot_epochs_image_temp_n_path = fname\n",
    "    \n",
    "def plot_final_temp_n(temp, one_spike=False, system_type='Mac'):\n",
    "    \"\"\" Plot all plots in one\"\"\"\n",
    "    import svgutils.transform as sg\n",
    "    from subprocess import Popen\n",
    "    import os\n",
    "    \n",
    "    fig = sg.SVGFigure(\"22cm\", \"24cm\")\n",
    "    plot1 = sg.fromfile(temp.templates_topo_temp_n_path).getroot()\n",
    "    #plot1 = sg.from_mpl(temp.templates_topo_temp_n_fig).getroot()\n",
    "    plot1.moveto(0, 0)\n",
    "    plot2 = sg.fromfile(temp.plot_evoked_joint_temp_n_grad_path).getroot()\n",
    "    #plot2 = sg.from_mpl(temp.plot_evoked_joint_temp_n_grad_fig).getroot()\n",
    "    plot2.moveto(30, 650)\n",
    "    plot2.scale_xy(0.7, 0.7)\n",
    "    \n",
    "    plot3 = sg.fromfile(temp.plot_epochs_image_temp_n_path).getroot()\n",
    "    plot3.moveto(450, 650)\n",
    "    plot3.scale_xy(0.7, 0.7)\n",
    "    \n",
    "    l = []\n",
    "    l.append(sg.TextElement(50,50, \"A\", size=12, weight=\"bold\"))\n",
    "    l.append(sg.TextElement(50,650, \"B\", size=12, weight=\"bold\"))\n",
    "    l.append(sg.TextElement(450,650, \"C\", size=12, weight=\"bold\"))\n",
    "    \n",
    "    fig.append([plot1, plot2, plot3])\n",
    "    fig.append(l)\n",
    "    fig.save(\"%s%s_fig_final.svg\"%(temp.svg_path, temp.temp_n))\n",
    "    \n",
    "    your_svg_input = \"%s%s_fig_final.svg\"%(temp.svg_path, temp.temp_n)\n",
    "    \n",
    "    if not one_spike:\n",
    "        your_png_output = \"%s%s_temp.png\"%(temp.png_path, temp.temp_n)\n",
    "    else:\n",
    "        os.makedirs(\"{}{}_temp\".format(temp.png_path, temp.temp_n),exist_ok=True)\n",
    "        your_png_output = \"{}{}_temp/{}.png\".format(temp.png_path, temp.temp_n,temp.one_spike_name)\n",
    "\n",
    "    if system_type == 'Mac':\n",
    "        x = Popen(['/Applications/Inkscape.app/Contents/Resources/bin/inkscape', your_svg_input, \\\n",
    "                   '--export-png=%s' % your_png_output, '-w3000 -h4500', '-b white'])\n",
    "    elif system_type == 'Windows':\n",
    "        x = Popen(['C:/Program Files/Inkscape/inkscape', your_svg_input, \\\n",
    "                   '--export-png=%s' % your_png_output, '-w2000 -h3000', '-b white'])\n",
    "    #try:\n",
    "    #    Templates._waitForResponse(x)\n",
    "    #except OSError:\n",
    "    #    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import scipy.io\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "block1 = scipy.io.loadmat('/Users/valery/MEG/Cases/B1C2/ASPIRE/results/\\\n",
    "sources_B1C2_ii_run1_raw_tsss_mc_art_corr_data_\\\n",
    "block001_ICA_based_grad.mat')\n",
    "\n",
    "block2 = scipy.io.loadmat('/Users/valery/MEG/Cases/B1C2/ASPIRE/results/\\\n",
    "sources_B1C2_ii_run1_raw_tsss_mc_art_corr_data_\\\n",
    "block002_ICA_based_grad.mat')\n",
    "block2['spikeind'][0,:] += 600_000\n",
    "spikes = block1['spikeind'][0,:].tolist() + block2['spikeind'][0,:].tolist()\n",
    "\n",
    "tsss_file = \"/Users/valery/MEG/Cases/B1C2/art_corr/B1C2_ii_run1_raw_tsss_mc_art_corr.fif\"\n",
    "data = mne.io.read_raw_fif(tsss_file, preload=False, verbose=False)\n",
    "#epochs = create_epochs(data, spikes, tmin=-0.2, tmax=0.2, picks_raw=True)\n",
    "\n",
    "#aligned_spikes = [epochs[i].events.tolist()[0][0]+\\\n",
    "#                  epochs[i].load_data().pick_types(meg='grad').filter(9.,100.).average().get_peak()[1]\\\n",
    "#                  for i in range(len(epochs))]\n",
    "aligned_spikes = [i+data.first_samp for i in spikes]\n",
    "results = pd.DataFrame(data=zip(spikes,aligned_spikes),columns=[\"Spikes\",\"Aligned_spikes\"])\n",
    "results.to_excel(\"/Users/valery/MEG/Cases/B1C2/results.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_circus_ASPIRE as run_circus\n",
    "import scipy.io\n",
    "import circus_templates_ASPIRE as circus_templates\n",
    "import mne\n",
    "import traceback\n",
    "import shutil\n",
    "\n",
    "dir_case = \"/Users/valery/MEG/Cases/B1C2/\"\n",
    "dir_SPC = dir_case + \"Spyking_circus/B1C2_B1C2_ii_run1_raw_tsss_mc_art_corr/\"\n",
    "SPC_fname = \"B1C2_ii_run1_raw_tsss_mc_art_corr.fif\"\n",
    "nmpy_file = 'B1C2_ii_run1_raw_tsss_mc_art_corr_0.npy'\n",
    "epochs_fname = dir_case +  \"art_corr/B1C2_ii_run1_raw_tsss_mc_art_corr_epochs.fif\"\n",
    "n_t = 100\n",
    "\n",
    "for cc_merge in [0.3]:\n",
    "    for mad in [7.0]:\n",
    "        try:\n",
    "            #shutil.copy(dir_SPC +'full_case/' + nmpy_file, dir_SPC)\n",
    "            sc = run_circus.Circus(dir_case, \"B1C2\", dir_SPC, SPC_fname, N_t=n_t, cc_merge=cc_merge, MAD=mad, cut_off=3)\n",
    "            sc.params_iterations(run_spc=True, only_fitting = False, sensors=['grad'])\n",
    "            \n",
    "            #shutil.copy(dir_SPC +'only_epochs/' + nmpy_file, dir_SPC)\n",
    "            #sc = run_circus.Circus(dir_case, \"B1C2\", dir_SPC, SPC_fname, N_t=n_t, cc_merge=0.97, MAD=mad, cut_off=9)\n",
    "            #sc.params_iterations(run_spc=True, only_fitting = True, sensors=['grad'])\n",
    "\n",
    "            for sens in ['grad']:\n",
    "                temp = circus_templates.Templates(dir_case, \"B1C2\", SPC_fname, sc.sensors_params[sens], sensors=sens, n_sp=3, N_t=n_t, cc_merge=0.3, MAD=mad)\n",
    "                #temp.plot_all_templates(epochs_fname, 'Mac')\n",
    "        except Exception: traceback.print_exc()\n",
    "\n",
    "        try:    \n",
    "            ## Select fitted spikes from SPC that was fitted on the same peak\n",
    "            import pandas as pd\n",
    "            results = pd.read_excel(\"/Users/valery/MEG/Cases/B1C2/results.xlsx\")\n",
    "\n",
    "            for i in range(len(results)):\n",
    "                around_spike = temp.templates[abs(temp.templates.Spiketimes-(results.Aligned_spikes[i]-39000))<100]\n",
    "                if not around_spike.empty:\n",
    "                    Amplitude =  find_nearest(around_spike.Amplitudes,1)\n",
    "                    Spiketime = temp.templates[temp.templates.Amplitudes==Amplitude].Spiketimes.tolist()[0]\n",
    "                    Template = temp.templates[temp.templates.Amplitudes==Amplitude].Template.tolist()[0]\n",
    "\n",
    "                    results.loc[i,'Difference'] = Spiketime - (results.Aligned_spikes[i]-39000)\n",
    "                    results.loc[i,'Template'] = temp.templates[temp.templates.Spiketimes==results.loc[i,'Difference']+(results.Aligned_spikes[i]-39000)].Template.tolist()[0]\n",
    "                    results.loc[i,'Template_n'] = int(results.loc[i,'Template'].split('_')[1])\n",
    "                    results.loc[i,'Amplitude'] = Amplitude\n",
    "\n",
    "        except Exception: traceback.print_exc()\n",
    "        try:\n",
    "            ## Plot average & individual spikes\n",
    "            import mne\n",
    "            import numpy as np\n",
    "\n",
    "            tsss_file = \"/Users/valery/MEG/Cases/B1C2/art_corr/B1C2_ii_run1_raw_tsss_mc_art_corr.fif\"\n",
    "            data = mne.io.read_raw_fif(tsss_file, preload=False, verbose=False)\n",
    "\n",
    "            godness_percentile = np.percentile(abs(results.dropna().Amplitude.to_numpy()-1),10) #10%\n",
    "            best_results = results[abs(results.Amplitude-1)<godness_percentile]\n",
    "\n",
    "            filter_value = (1.0,45.0)\n",
    "\n",
    "            for temp_n in best_results.Template_n.unique().tolist():\n",
    "                results_temp_n = best_results[best_results.Template_n == temp_n]\n",
    "                aligned_spikes = (results_temp_n.Aligned_spikes + results_temp_n.Difference).tolist()\n",
    "\n",
    "                epochs = create_epochs(data, aligned_spikes, tmin=-0.5, tmax=0.5, picks_raw=True,no_first_sample=True)\n",
    "\n",
    "                temp.data_info = epochs.info\n",
    "                temp.data_info_sens = epochs[0].load_data().pick_types(meg=temp.sensors, eeg=False,stim=False, eog=False).info\n",
    "                temp.temp_n = int(temp_n)\n",
    "\n",
    "                epochs.load_data().pick_types(meg=temp.sensors, eeg=False,stim=False, eog=False).filter(filter_value[0], filter_value[1], fir_design='firwin')\n",
    "                temp_evocked = temp.templates_topo_temp_n(return_temp=True)\n",
    "\n",
    "                ## Plot each spike\n",
    "                for i in range(len(epochs)):\n",
    "                    ep = epochs[i]\n",
    "                    time = ep.events[0][0]\n",
    "                    ev = ep.average()\n",
    "                    amplitude = results_temp_n.loc[(results_temp_n.Aligned_spikes+results_temp_n.Difference).round()==time,'Amplitude'].values[0]\n",
    "                    plot_epoch_image(epochs[i])\n",
    "                    temp_evocked = temp.templates_topo_temp_n(return_temp=True)\n",
    "                    plot_topomap(temp_evocked, ev, temp.svg_path, temp.temp_n, int(time), amplitude, one_spike=True)\n",
    "                    plot_joint(ev)\n",
    "                    plot_final_temp_n(temp, one_spike=True)\n",
    "\n",
    "                ## Plot average\n",
    "                temp_evocked = temp.templates_topo_temp_n(return_temp=True)\n",
    "                file_name = 'average_{}_spikes_filt_{}'.format(len(results_temp_n),filter_value)\n",
    "                plot_epoch_image(epochs)\n",
    "                evocked = epochs.average()\n",
    "\n",
    "                amplitude = results_temp_n.Amplitude.mean()   \n",
    "                plot_topomap(temp_evocked, evocked, temp.svg_path, temp.temp_n, file_name, amplitude)\n",
    "                plot_joint(evocked)\n",
    "\n",
    "                plot_final_temp_n(temp)\n",
    "\n",
    "            \n",
    "            results.to_excel(temp.waveforms_path + '/results.xlsx')\n",
    "        except Exception: traceback.print_exc()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select fitted spikes from SPC that was fitted on the same peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "results = pd.read_excel(\"/Users/valery/MEG/Cases/B1C2/results.xlsx\")\n",
    "\n",
    "for i in range(len(results)):\n",
    "    around_spike = temp.templates[abs(temp.templates.Spiketimes-(results.Aligned_spikes[i]-39000))<100]\n",
    "    if not around_spike.empty:\n",
    "        Amplitude =  find_nearest(around_spike.Amplitudes,1)\n",
    "        Spiketime = temp.templates[temp.templates.Amplitudes==Amplitude].Spiketimes.tolist()[0]\n",
    "        Template = temp.templates[temp.templates.Amplitudes==Amplitude].Template.tolist()[0]\n",
    "\n",
    "        results.loc[i,'Difference'] = Spiketime - (results.Aligned_spikes[i]-39000)\n",
    "        results.loc[i,'Template'] = temp.templates[temp.templates.Spiketimes==results.loc[i,'Difference']+(results.Aligned_spikes[i]-39000)].Template.tolist()[0]\n",
    "        results.loc[i,'Template_n'] = int(results.loc[i,'Template'].split('_')[1])\n",
    "        results.loc[i,'Amplitude'] = Amplitude\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot average & individual spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import mne\n",
    "import numpy as np\n",
    "\n",
    "godness_percentile = np.percentile(abs(results.dropna().Amplitude.to_numpy()-1),10) #10%\n",
    "tsss_file = \"/Users/valery/MEG/Cases/B1C2/art_corr/B1C2_ii_run1_raw_tsss_mc_art_corr.fif\"\n",
    "data = mne.io.read_raw_fif(tsss_file, preload=False, verbose=False)\n",
    "best_results = results[abs(results.Amplitude-1)<godness_percentile]\n",
    "filter_value = (1.0,45.0)\n",
    "\n",
    "for temp_n in best_results.Template_n.unique().tolist():\n",
    "    results_temp_n = best_results[best_results.Template_n == temp_n]\n",
    "    aligned_spikes = (results_temp_n.Aligned_spikes + results_temp_n.Difference).tolist()\n",
    "    \n",
    "    epochs = create_epochs(data, aligned_spikes, tmin=-0.5, tmax=0.5, picks_raw=True,no_first_sample=True)\n",
    "    \n",
    "    temp.data_info = epochs.info\n",
    "    temp.data_info_sens = epochs[0].load_data().pick_types(meg=temp.sensors, eeg=False,stim=False, eog=False).info\n",
    "    temp.temp_n = int(temp_n)\n",
    "    \n",
    "    epochs.load_data().pick_types(meg=temp.sensors, eeg=False,stim=False, eog=False).filter(filter_value[0], filter_value[1], fir_design='firwin')\n",
    "    temp_evocked = temp.templates_topo_temp_n(return_temp=True)\n",
    "    \n",
    "    ## Plot each spike\n",
    "    for i in range(len(epochs)):\n",
    "        ep = epochs[i]\n",
    "        time = ep.events[0][0]\n",
    "        ev = ep.average()\n",
    "        amplitude = results_temp_n.loc[(results_temp_n.Aligned_spikes+results_temp_n.Difference).round()==time,'Amplitude'].values[0]\n",
    "        plot_epoch_image(epochs[i])\n",
    "        temp_evocked = temp.templates_topo_temp_n(return_temp=True)\n",
    "        plot_topomap(temp_evocked, ev, temp.svg_path, temp.temp_n, int(time), amplitude, one_spike=True)\n",
    "        plot_joint(ev)\n",
    "        plot_final_temp_n(temp, one_spike=True)\n",
    "        \n",
    "    ## Plot average\n",
    "    temp_evocked = temp.templates_topo_temp_n(return_temp=True)\n",
    "    file_name = 'average_{}_spikes_filt_{}'.format(len(results_temp_n),filter_value)\n",
    "    plot_epoch_image(epochs)\n",
    "    evocked = epochs.average()\n",
    "    \n",
    "    amplitude = results_temp_n.Amplitude.mean()   \n",
    "    plot_topomap(temp_evocked, evocked, temp.svg_path, temp.temp_n, file_name, amplitude)\n",
    "    plot_joint(evocked)\n",
    "    \n",
    "    plot_final_temp_n(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import scipy.io\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#T_spikes = scipy.io.loadmat('/Users/valery/MEG/Cases/B1C2/tommaso_visual_marking_B1C2_events_block001.mat')\n",
    "results = pd.read_excel('/Users/valery/MEG/Cases/B1C2/Spyking_circus/B1C2_B1C2_ii_run1_raw_tsss_mc_art_corr/cut_off_7_spike_thresh_7.0_N_t_100_grad_(B1C2_ii_run1_raw_tsss_mc_art_corr_0)_cc_merge_0.3_sensitivity_3/Templates_waveforms/results.xlsx')\n",
    "tsss_file = \"/Users/valery/MEG/Cases/B1C2/art_corr/B1C2_ii_run1_raw_tsss_mc_art_corr.fif\"\n",
    "data = mne.io.read_raw_fif(tsss_file, preload=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_spikes = [ 39.264,  40.731,  41.631,  72.827,  73.303,  74.078,  74.564,\n",
    "         75.142,  75.99 ,  76.476,  76.827,  77.396,  87.175,  88.157,\n",
    "         88.704,  89.139,  90.131, 101.719, 102.949, 103.881, 104.439,\n",
    "        104.811, 105.266, 105.295, 106.754, 111.861, 111.902, 128.441,\n",
    "        129.051, 129.505, 130.053, 130.446, 130.963, 137.589, 137.972,\n",
    "        149.239, 156.04 , 157.973, 158.791, 169.406, 173.241, 173.614,\n",
    "        217.463, 218.424, 220.336, 224.936, 232.244, 232.73 , 234.498,\n",
    "        251.067, 251.646, 252.132, 252.535, 253.135, 253.527, 265.095,\n",
    "        265.456, 266.077, 277.385, 278.243, 279.256, 290.038, 291.04 ,\n",
    "        291.536, 295.806, 300.654, 301.512, 301.977, 324.822, 326.114,\n",
    "        327.292, 333.566, 334.838, 335.676, 336.223, 341.133, 341.536,\n",
    "        341.877, 393.707, 403.517, 422.66 , 460.111, 460.928, 463.481,\n",
    "        463.791, 464.101, 470.314, 470.831, 480.185, 480.557, 483.959,\n",
    "        486.315, 495.111, 495.536, 495.556, 503.143, 503.671, 504.146,\n",
    "        504.663, 505.149, 505.614, 506.038, 542.096, 542.613, 543.894,\n",
    "        616.532, 616.542, 619.282, 621.669]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "godness_percentile = np.percentile(abs(results.dropna().Amplitude.to_numpy()-1),100) #10%\n",
    "best_results = results[abs(results.Amplitude-1)<godness_percentile]\n",
    "ICA_spikes = results.Aligned_spikes.tolist() \n",
    "V_spikes = (best_results.Aligned_spikes+best_results.Difference).tolist()\n",
    "#scipy.io.savemat('/Users/valery/MEG/Cases/B1C2/SPC_B1C2_full.mat',{'SPC':V_spikes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(V_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib qt5\n",
    "%matplotlib qt5\n",
    "\n",
    "\n",
    "plot_sensors = 'grad' # 'mag', 'grad' or True (all sensors)\n",
    "window = 10 #in seconds\n",
    "start = 988.0 #start point in seconds\n",
    "\n",
    "tsss_file = \"/Users/valery/MEG/Cases/B1C2/art_corr/B1C2_ii_run1_raw_tsss_mc_art_corr.fif\"\n",
    "data = mne.io.read_raw_fif(tsss_file, preload=False, verbose=False)\n",
    "\n",
    "onset = [i for i in T_spikes]\n",
    "data.annotations.append(onset, np.repeat(0.00001, len(onset)), ['T']*len(onset))\n",
    "\n",
    "onset = [i/1000 for i in ICA_spikes]\n",
    "data.annotations.append(onset, np.repeat(0.00001, len(onset)), ['I']*len(onset))\n",
    "\n",
    "onset = [i/1000 for i in V_spikes]\n",
    "labels = [str(i) for i in best_results.Template_n]\n",
    "data.annotations.append(onset, np.repeat(0.00001, len(onset)), labels)\n",
    "\n",
    "data.plot(duration=window, group_by='position', start=start, lowpass=70, highpass=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_res_ASPIRE = '/Users/valery/MEG/Cases/B1C2/ASPIRE/results/sources__befor_thershold_B1C2_ii_run1_raw_tsss_mc_art_corr_data_block00'\n",
    "detections = {}\n",
    "block1 = scipy.io.loadmat(path_res_ASPIRE + '1_ICA_based_grad.mat')\n",
    "#block2 = scipy.io.loadmat(path_res_ASPIRE + '2_ICA_based_grad.mat')\n",
    "#block2['spike_ind'][0,:] += 600_000\n",
    "#detections['ICA'] = pd.DataFrame(data=np.array([block1['spike_ind'][0,:].tolist()+block2['spike_ind'][0,:].tolist(),\n",
    "#                            block1['ValMax'][0,:].tolist()+block2['ValMax'][0,:].tolist()]).T, columns= ['spike_ind','ValMax'])\n",
    "detections['ICA'] = pd.DataFrame(data=np.array([block1['spike_ind'][0,:].tolist(),block1['ValMax'][0,:].tolist()]).T, \n",
    "                                 columns= ['spike_ind','ValMax'])\n",
    "detections['ICA'].spike_ind += 39000\n",
    "\n",
    "block1 = scipy.io.loadmat(path_res_ASPIRE + '1_SpyCir_based_grad.mat')\n",
    "#block2 = scipy.io.loadmat(path_res_ASPIRE + '2_SpyCir_based_grad.mat')\n",
    "#block2['spike_ind'][:,0] += 600_000\n",
    "#detections['SPC'] = pd.DataFrame(data=np.array([block1['spike_ind'][:,0].tolist()+block2['spike_ind'][:,0].tolist(),\n",
    "#                            block1['ValMax'][0,:].tolist()+block2['ValMax'][0,:].tolist()]).T, columns= ['spike_ind','ValMax'])\n",
    "\n",
    "detections['SPC'] = pd.DataFrame(data=np.array([block1['spike_ind'][:,0].tolist(),block1['ValMax'][0,:].tolist()]).T, \n",
    "                                 columns= ['spike_ind','ValMax'])\n",
    "detections['SPC'].spike_ind += 39000\n",
    "res_csv_spc = pd.read_csv('/Users/valery/MEG/Cases/B1C2/ASPIRE/detections/Templates_B1C2_ii_run1_raw_tsss_mc_grad.csv')\n",
    "res_csv_spc.sort_values('Spiketimes', inplace=True)\n",
    "res_csv_spc.Spiketimes += 39000\n",
    "#detections['SPC']['Amplitudes'] = 0\n",
    "for i in range(len(detections['SPC'].spike_ind)):\n",
    "    detections['SPC'].loc[i,'Amplitudes'] = res_csv_spc[res_csv_spc.Spiketimes==detections['SPC'].spike_ind[i]].Amplitudes.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_spikes = [ 39.264,  40.731,  41.631,  72.827,  73.303,  74.078,  74.564,\n",
    "         75.142,  75.99 ,  76.476,  76.827,  77.396,  87.175,  88.157,\n",
    "         88.704,  89.139,  90.131, 101.719, 102.949, 103.881, 104.439,\n",
    "        104.811, 105.266, 105.295, 106.754, 111.861, 111.902, 128.441,\n",
    "        129.051, 129.505, 130.053, 130.446, 130.963, 137.589, 137.972,\n",
    "        149.239, 156.04 , 157.973, 158.791, 169.406, 173.241, 173.614,\n",
    "        217.463, 218.424, 220.336, 224.936, 232.244, 232.73 , 234.498,\n",
    "        251.067, 251.646, 252.132, 252.535, 253.135, 253.527, 265.095,\n",
    "        265.456, 266.077, 277.385, 278.243, 279.256, 290.038, 291.04 ,\n",
    "        291.536, 295.806, 300.654, 301.512, 301.977, 324.822, 326.114,\n",
    "        327.292, 333.566, 334.838, 335.676, 336.223, 341.133, 341.536,\n",
    "        341.877, 393.707, 403.517, 422.66 , 460.111, 460.928, 463.481,\n",
    "        463.791, 464.101, 470.314, 470.831, 480.185, 480.557, 483.959,\n",
    "        486.315, 495.111, 495.536, 495.556, 503.143, 503.671, 504.146,\n",
    "        504.663, 505.149, 505.614, 506.038, 542.096, 542.613, 543.894,\n",
    "        616.532, 616.542, 619.282, 621.669]\n",
    "\n",
    "visual_track = pd.DataFrame(data=[i*1000 for i in T_spikes],columns=['Visual'])\n",
    "#ROC_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity&Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(array, value):\n",
    "    import numpy as np\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "\n",
    "def sensitivity_precision(visual_track, det, detections): \n",
    "    win = 100 #ms around detections\n",
    "    \n",
    "    for i in range(len(visual_track.Visual)):\n",
    "        if abs(find_nearest(detections[det].spike_ind, visual_track.loc[i,'Visual']) - visual_track.loc[i,'Visual']) <= win:\n",
    "            visual_track.loc[i,det] = 1\n",
    "        else:\n",
    "            visual_track.loc[i,det] = 0    \n",
    "    \n",
    "    TP = len(visual_track[det]) - len(visual_track[visual_track[det]==0.0][det])\n",
    "    FN = len(visual_track[det]) - len(visual_track[visual_track[det]==1.0][det])\n",
    "    \n",
    "    FP = 0\n",
    "    for i in range(len(detections[det].spike_ind)):\n",
    "        if abs(find_nearest(visual_track['Visual'],detections[det].loc[i,'spike_ind']) - detections[det].loc[i,'spike_ind']) > win:\n",
    "            FP += 1       \n",
    "    if TP!=0:\n",
    "        sensitivity = TP/(TP + FN)\n",
    "        precision = TP/(TP + FP)\n",
    "    else:\n",
    "        sensitivity = 0\n",
    "        precision = 0\n",
    "    return sensitivity, precision\n",
    "\n",
    "def apply_threshold(detections, det, thr_name):\n",
    "    import numpy as np\n",
    "    for percentile in range(0,100,5):\n",
    "        if thr_name != 'Amplitudes':\n",
    "            threshold = np.percentile(detections[det][thr_name].to_numpy(),percentile) #10%\n",
    "            detections['{}_{}_{}'.format(det, thr_name, percentile)] = detections[det][detections[det][thr_name]>=threshold].reset_index(drop=True)\n",
    "        else:\n",
    "            threshold = np.percentile(-abs(1-detections[det][thr_name].to_numpy()), percentile)\n",
    "            detections['{}_{}_{}'.format(det, thr_name, percentile)] = detections[det][-abs(1-detections[det][thr_name])>=threshold].reset_index(drop=True)\n",
    "\n",
    "def plot_roc(visual_track, det_list, detections, thr_name):\n",
    "    res_roc = pd.DataFrame()\n",
    "    n = 0\n",
    "    for det in det_list:\n",
    "        for perc in range(0,100,5):\n",
    "            sens, prec = sensitivity_precision(visual_track, '{}_{}_{}'.format(det, thr_name, perc), detections)\n",
    "            res_roc.loc[n, 'Percentile'] = perc\n",
    "            res_roc.loc[n, 'ROC type'] = 'Sensitivity '+det\n",
    "            res_roc.loc[n, 'Sensitivity and Precision'] = sens\n",
    "            res_roc.loc[n, 'N_spikes'] = len(detections['{}_{}_{}'.format(det, thr_name, perc)])\n",
    "            n += 1\n",
    "            res_roc.loc[n, 'Percentile'] = perc\n",
    "            res_roc.loc[n, 'ROC type'] = 'Precision '+det\n",
    "            res_roc.loc[n, 'Sensitivity and Precision'] = prec\n",
    "            res_roc.loc[n, 'N_spikes'] = len(detections['{}_{}_{}'.format(det, thr_name, perc)])\n",
    "            n += 1\n",
    "    return res_roc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.017151848937844216)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity_precision(visual_track, 'SPC', detections)\n",
    "#visual_track[(visual_track.ICA == 0)|(visual_track.SPC == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_threshold(detections, 'ICA', 'ValMax')\n",
    "apply_threshold(detections, 'SPC', 'ValMax')\n",
    "res_fitting_thesholds = plot_roc(visual_track, ['ICA','SPC'], detections, 'ValMax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_threshold(detections, 'SPC', 'Amplitudes')\n",
    "res_SPC_amplitudes = plot_roc(visual_track, ['SPC'], detections, 'Amplitudes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n =0\n",
    "sel_thr = '_Amplitudes_20'\n",
    "detections['Overlap'] = pd.DataFrame()\n",
    "for i in detections['ICA'].spike_ind:\n",
    "    if not detections['SPC'+sel_thr][abs(detections['SPC'+sel_thr].spike_ind - i)<20].empty:\n",
    "        ind_max = detections['SPC'+sel_thr][abs(detections['SPC'+sel_thr].spike_ind - i)<20].sort_values('Amplitudes').index[0]\n",
    "        detections['Overlap'].loc[n,'spike_ind'] = detections['SPC'+sel_thr].loc[ind_max, 'spike_ind']\n",
    "        detections['Overlap'].loc[n,'ValMax'] = detections['SPC'+sel_thr].loc[ind_max, 'ValMax']\n",
    "        detections['Overlap'].loc[n,'Amplitudes'] = detections['SPC'+sel_thr].loc[ind_max, 'Amplitudes']\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_threshold(detections, 'Overlap', 'ValMax')\n",
    "res_Overlap_fitting_thesholds_best_spc = plot_roc(visual_track, ['Overlap'], detections, 'ValMax')\n",
    "apply_threshold(detections, 'Overlap', 'Amplitudes')\n",
    "res_Overlap_amplitudes_thesholds_best_spc = plot_roc(visual_track, ['Overlap'], detections, 'Amplitudes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "fig, axs = plt.subplots(2, 2,figsize=(13,13))\n",
    "sns.lineplot(x=\"Percentile\", y='Sensitivity and Precision', hue='ROC type', data=res_fitting_thesholds, ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Fitting threshold (raw detections)')\n",
    "\n",
    "\n",
    "sns.lineplot(x=\"Percentile\", y='Sensitivity and Precision', hue='ROC type', data=res_SPC_amplitudes, ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Amplitudes threshold (SPC detections)')\n",
    "\n",
    "sns.lineplot(x=\"Percentile\", y='Sensitivity and Precision', hue='ROC type', data=res_Overlap_fitting_thesholds_best_spc, ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Fitting threshold (overlap between ICA and SPC)\\nonly the best 20% SPC detections were used')\n",
    "\n",
    "sns.lineplot(x=\"Percentile\", y='Sensitivity and Precision', hue='ROC type', data=res_Overlap_amplitudes_thesholds_best_spc, ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Amplitudes threshold (overlap between ICA and SPC)\\nonly the best 20% SPC detections were used')\n",
    "\n",
    "plt.suptitle('Sensitivity = TP/(TP + FN)\\nPrecision = TP/(TP + FP)\\nParameters for SPC: filter 9-100Hz, spike threshold 5MAD, template width 80ms, cc_merge = 0.9\\nThe number of the manual spikes 109')\n",
    "plt.savefig('/Users/valery/MEG/Cases/B1C2/ROC_thresholds.png', dpi=400)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib qt5\n",
    "%matplotlib qt5\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "plot_sensors = 'grad' # 'mag', 'grad' or True (all sensors)\n",
    "window = 10 #in seconds\n",
    "start = 988.0 #start point in seconds\n",
    "\n",
    "tsss_file = \"/Users/valery/MEG/Cases/B1C2/art_corr/B1C2_ii_run1_raw_tsss_mc_art_corr.fif\"\n",
    "data = mne.io.read_raw_fif(tsss_file, preload=False, verbose=False)\n",
    "\n",
    "onset = [i for i in T_spikes]\n",
    "data.annotations.append(onset, np.repeat(0.00001, len(onset)), ['T']*len(onset))\n",
    "\n",
    "onset = [i/1000 for i in detections['Overlap_ValMax_50'].spike_ind]\n",
    "data.annotations.append(onset, np.repeat(0.00001, len(onset)), ['O']*len(onset))\n",
    "\n",
    "#onset = [i/1000 for i in V_spikes]\n",
    "#labels = [str(i) for i in best_results.Template_n]\n",
    "#data.annotations.append(onset, np.repeat(0.00001, len(onset)), labels)\n",
    "\n",
    "data.plot(duration=window, group_by='position', start=start, lowpass=70, highpass=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6299416550000001"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(-abs(1-detections['SPC']['Amplitudes'].to_numpy()),5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50      0.918396\n",
       "58      0.923617\n",
       "61      0.923371\n",
       "63      0.917635\n",
       "64      0.919504\n",
       "          ...   \n",
       "2139    0.914062\n",
       "2141    0.922433\n",
       "2146    0.910942\n",
       "2253    0.921052\n",
       "2258    0.912924\n",
       "Name: ValMax, Length: 114, dtype: float64"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections['ICA'][detections['ICA']['ValMax']<=0.9240047691429929]['ValMax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9731173930080393"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections['ICA_ValMax_95']['ValMax'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
